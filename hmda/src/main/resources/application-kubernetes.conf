include "persistence.conf"
include "hmda.conf"
include "cors.conf"

akka {


  diagnostics {
    starvation-detector {
      check-interval = 1s
//      initial-delay = 8s TODO: bump it up before master
      max-delay-warning-threshold = 100 ms
      warning-interval = 10 seconds
    }
  }

  loglevel = INFO

  http.server.default-host-header = "cfpb.gov"
  http.parsing.max-content-length = 1G
  blocking-quality-dispatcher {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 2
      parallelism-factor = 2.0
    }
    throughput = 1000
  }
  actor {
    provider = "cluster"
    timeout = 8
  }

  remote {
    maximum-payload-bytes = 30000000 bytes
    netty.tcp {
      message-frame-size =  30000000b
      send-buffer-size =  30000000b
      receive-buffer-size =  30000000b
      maximum-frame-size = 30000000b
    }
  }

  discovery {
    kubernetes-api {
      pod-label-selector = "actorSystemName=hmda2"
      pod-namespace = "default"
      pod-namespace = ${?KUBERNETES_HMDA_POD_NAMESPACE}
    }
  }

  management {
    http {
      port = 8558
    }

    cluster {

      sharding {
//        number-of-shards = 500
        passivate-idle-entity-after = 15 minutes
      }

      failure-detector.acceptable-heartbeat-pause = 12s
      bootstrap {

        contact-point-discovery {
          discovery-method = kubernetes-api
          service-name = "hmda-api"
          service-name = ${?KUBERNETES_HMDA_SERVICE_NAME}
          service-namespace = "default.svc.cluster.local"
          service-namespace = ${?KUBERNETES_HMDA_SERVICE_NAMESPACE}
          stable-margin = 5 seconds
        }

        #contact-point {
        # currently this port HAS TO be the same as the `akka.management.http.port`
        # it would not have to be once we implement the SRV record watching, since then we could potentially
        # get the ports from the DNS records.
        #  fallback-port = 8558
        #}
      }
    }

  }
}

cinnamon.application = "hmda-platform"

cinnamon.akka {
  // monitor all actors
  // see https://developer.lightbend.com/docs/telemetry/current/instrumentations/akka/akka.html
  actors {
    "/user/*" {
      report-by = class
      metrics {
        mailbox-size {
          sampling-period = 100ms
        }
        stash-size {
          sampling-period = 100ms
        }
      }
    }
  }

  // monitor the named-example stream that we have explicitly instrumented
  // see https://developer.lightbend.com/docs/telemetry/current/instrumentations/akka-streams/akka-streams.html
  streams {
    "hmda.persistence.submission.*" {
      report-by = name
    }
  }

  persistence.entities {
    "*" {
      report-by = class
      command-type = on
    }
  }

  //  https://developer.lightbend.com/docs/telemetry/current/instrumentations/akka/akka.html#actor-remote-metrics
  remote.serialization-timing = on
  remote.failure-detector-metrics = on

//  https://developer.lightbend.com/docs/telemetry/current/instrumentations/akka/akka.html#cluster-events
  cluster.domain-events = on
  cluster.member-events = on
  cluster.shard-region-info = on




  // monitor akka http paths
  // see https://developer.lightbend.com/docs/telemetry/current/instrumentations/akka-http/akka-http-configuration.html#example-configuration-1
  http {
    servers {
      "*:*" {
        paths {
          "*" {
            metrics = on
          }
        }
      }
    }
  }
}
// expose the HTTP metrics server that Prometheus will scrape to gather metrics
cinnamon.prometheus {
  http-server {
    port = 9001
    daemon = false
  }

  // runs on port 9001
  exporters += http-server
}